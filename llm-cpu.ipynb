{"cells":[{"cell_type":"markdown","metadata":{},"source":["### LLM Inference CPU"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:19:11.090515Z","iopub.status.busy":"2025-10-30T19:19:11.090284Z","iopub.status.idle":"2025-10-30T19:20:55.162447Z","shell.execute_reply":"2025-10-30T19:20:55.160408Z","shell.execute_reply.started":"2025-10-30T19:19:11.090494Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model on CPU...\n","\n","--- Generating response ---\n","\n","--- Model Output ---\n","\n","MLOps is a process of automating the deployment, monitoring, and maintenance of machine learning (ML) models. It involves the following steps:\n","\n","1. Model Development: The first step in MLOps is model development. This involves creating the training data, selecting the model architecture, and optimizing the hyperparameters.\n","\n","2. Model Deployment: Once the model is developed, it needs to be deployed to production. This involves setting up a production environment, configuring the deployment pipeline, and monitoring the model's performance.\n","\n","3. Monitoring and Maintenance: The model needs to be monitored and maintained throughout its lifecycle. This involves monitoring the model's performance, detecting errors, and fixing them.\n","\n","Here are three real-world examples of MLOps:\n","\n","1. Netflix: Netflix is a streaming service that uses machine learning to recommend movies and TV shows to its users. Netfli\n","\n","--- Stats ---\n","Prompt tokens: 55\n","Generated tokens: 200\n","Total time: 41.968 s\n","Throughput: 4.77 tokens/s on CPU\n"]}],"source":["import time\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer # Import TextStreamer\n","\n","# --- CONFIG ---\n","MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","QUERY = \"Explain MLOps briefly and give 3 real-world examples.\"\n","device_choice = \"cpu\"   # \"cpu\" | \"cuda\" | \"tpu\"\n","\n","# --- DEVICE SETUP ---\n","if device_choice not in [\"cpu\", \"cuda\", \"tpu\"]:\n","    raise ValueError(\"device_choice must be one of 'cpu', 'cuda', or 'tpu'.\")\n","\n","if device_choice == \"cuda\":\n","    if not torch.cuda.is_available():\n","        raise EnvironmentError(\"CUDA not available — please switch to CPU or TPU.\")\n","    device = torch.device(\"cuda\")\n","elif device_choice == \"tpu\":\n","    try:\n","        import torch_xla.core.xla_model as xm\n","        device = xm.xla_device()\n","        print(\"Using TPU device.\")\n","    except Exception as e:\n","        raise EnvironmentError(\"TPU runtime not found. Did you enable TPU in Colab?\") from e\n","else:\n","    device = torch.device(\"cpu\")\n","\n","# --- LOAD TOKENIZER SAFELY (no chat-template fetch) ---\n","tokenizer = AutoTokenizer.from_pretrained(\n","    MODEL_ID,\n","    use_fast=True,\n","    trust_remote_code=True,\n","    local_files_only=False,\n",")\n","# Prevent future chat-template lookup attempts\n","if hasattr(tokenizer, \"_set_chat_template\"):\n","    tokenizer._set_chat_template(None, \"remove\")\n","\n","# --- LOAD MODEL ON SELECTED DEVICE ---\n","print(f\"Loading model on {device_choice.upper()}...\")\n","\n","dtype = torch.float16 if device_choice == \"cuda\" else torch.float32\n","\n","if device_choice == \"cuda\":\n","    try:\n","        from transformers import BitsAndBytesConfig\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            quantization_config=bnb_config,\n","            device_map={\"\": \"cuda\"},\n","            torch_dtype=torch.float16,\n","            trust_remote_code=True,\n","        )\n","        print(\"Model loaded in 4-bit quantized GPU mode.\")\n","    except Exception:\n","        print(\"bitsandbytes not installed — loading model in full precision.\")\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            torch_dtype=torch.float16,\n","            trust_remote_code=True,\n","        ).to(device)\n","elif device_choice == \"tpu\":\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=torch.float32,\n","        trust_remote_code=True,\n","    ).to(device)\n","else:\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=torch.float32,\n","        trust_remote_code=True,\n","    ).to(device)\n","\n","model.eval()\n","\n","# --- PROMPT PREPARATION ---\n","system_prompt = \"You are a concise and knowledgeable AI assistant.\"\n","prompt = (\n","    f\"<|system|>\\n{system_prompt}\\n</s>\\n\"\n","    f\"<|user|>\\n{QUERY}\\n</s>\\n\"\n","    f\"<|assistant|>\\n\"\n",")\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","# --- GENERATION ---\n","gen_kwargs = dict(max_new_tokens=200, temperature=0.2, top_p=0.95, do_sample=True)\n","\n","print(\"\\n--- Generating response ---\")\n","start = time.perf_counter()\n","\n","# Initialize the TextStreamer for word-by-word printing\n","# We skip the prompt's tokens in the streamer\n","streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","\n","# Variables to store generated tokens count and output text for stats\n","output_ids = None\n","generated_text = \"\"\n","\n","with torch.no_grad():\n","    if device_choice == \"tpu\":\n","        # TPU generation usually doesn't support the streamer seamlessly with the same API\n","        # Fall back to the original method for output_ids and decode/print afterwards\n","        output_ids = model.generate(**inputs, **gen_kwargs)\n","        import torch_xla.core.xla_model as xm\n","        xm.mark_step()\n","        # Decode and print the text for TPU case\n","        generated_text = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n","        print(\"\\n--- Model Output ---\\n\")\n","        print(generated_text.strip())\n","    else:\n","        # For CPU/CUDA, use the streamer to print output word-by-word\n","        print(\"\\n--- Model Output ---\\n\")\n","        # Generate and stream output. The output_ids returned will be the full sequence.\n","        output_ids = model.generate(\n","            **inputs,\n","            **gen_kwargs,\n","            streamer=streamer, # Pass the streamer\n","        )\n","        # Since the streamer prints, we just need to decode the output_ids\n","        # to get the full text for the stats calculation if needed.\n","        # The streamer has already printed the text.\n","        generated_text = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n","\n","\n","end = time.perf_counter()\n","\n","# --- TIMING ---\n","elapsed_s = end - start\n","# Use output_ids.shape[1] if available, otherwise assume length of generated_text for approximation (less accurate)\n","generated_tokens = output_ids.shape[1] - inputs[\"input_ids\"].shape[1] if output_ids is not None else len(tokenizer.encode(generated_text, add_special_tokens=False))\n","tps = generated_tokens / elapsed_s if elapsed_s > 0 else float(\"nan\")\n","\n","print(\"\\n--- Stats ---\")\n","print(f\"Prompt tokens: {inputs['input_ids'].shape[1]}\")\n","print(f\"Generated tokens: {generated_tokens}\")\n","print(f\"Total time: {elapsed_s:.3f} s\")\n","print(f\"Throughput: {tps:.2f} tokens/s on {device_choice.upper()}\")"]}],"metadata":{"kaggle":{"accelerator":"tpuV5e8","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
