{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CPU & GPU & TPU Roles in MLOps \n","\n","### Serdar Biçici 150210331"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:01:05.518551Z","iopub.status.busy":"2025-10-30T19:01:05.518388Z","iopub.status.idle":"2025-10-30T19:01:09.813641Z","shell.execute_reply":"2025-10-30T19:01:09.812977Z","shell.execute_reply.started":"2025-10-30T19:01:05.518533Z"},"trusted":true},"outputs":[],"source":["# set seed\n","import random\n","random.seed(42)\n","import numpy as np\n","np.random.seed(42)\n","import torch\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n"]},{"cell_type":"markdown","metadata":{},"source":["## CPU Experiments"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Networks"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:01:12.898787Z","iopub.status.busy":"2025-10-30T19:01:12.898431Z","iopub.status.idle":"2025-10-30T19:03:47.333027Z","shell.execute_reply":"2025-10-30T19:03:47.332000Z","shell.execute_reply.started":"2025-10-30T19:01:12.898760Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 33.8MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 1.12MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 9.88MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 6.86MB/s]"]},{"name":"stdout","output_type":"stream","text":["Device: CPU  |  dtype: torch.float32  |  epochs: 1\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 1] Train loss: 0.2545  |  Train acc: 92.19%  |  Time: 137.93s\n","\n","Test accuracy: 98.25%\n","Inference time (test set): 10.324 s for 10000 samples\n","Throughput: 968.65 samples/s on CPU\n"]}],"source":["# pip install -q \"torch>=2.1\" \"torchvision>=0.16\" \"transformers>=4.41\"  # transformers not required; kept for parity\n","# For TPU: pip install -q torch_xla[tpu]~=2.1\n","\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","# --- CONFIG ---\n","device_choice = \"cpu\"   # \"cpu\" | \"cuda\" | \"tpu\"\n","batch_size = 128\n","epochs = 1              # keep small for quick sanity runs; increase as needed\n","lr = 1e-3\n","seed = 42\n","\n","torch.manual_seed(seed)\n","\n","# --- Device selection & checks ---\n","xm = None\n","if device_choice not in [\"cpu\", \"cuda\", \"tpu\"]:\n","    raise ValueError(\"device_choice must be 'cpu', 'cuda', or 'tpu'.\")\n","\n","if device_choice == \"cuda\":\n","    if not torch.cuda.is_available():\n","        raise EnvironmentError(\"CUDA not available but 'cuda' was requested.\")\n","    device = torch.device(\"cuda\")\n","    torch.backends.cudnn.benchmark = True\n","elif device_choice == \"tpu\":\n","    try:\n","        import torch_xla.core.xla_model as xm\n","        device = xm.xla_device()\n","    except Exception as e:\n","        raise EnvironmentError(\n","            \"TPU/XLA not available. Ensure TPU runtime and torch_xla are installed.\"\n","        ) from e\n","else:\n","    device = torch.device(\"cpu\")\n","\n","dtype = torch.float32  # use fp32 for reliability across backends\n","\n","# --- Data ---\n","tfm = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,)),  # standard MNIST normalization\n","])\n","\n","train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n","test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=tfm)\n","\n","# sensible loader defaults across devices\n","num_workers = 2 if device_choice in [\"cpu\", \"cuda\"] else 0\n","pin_memory = True if device_choice == \"cuda\" else False\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n","                          num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n","test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n","                          num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n","\n","# --- Model ---\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)   # 28x28 -> 28x28\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 28x28 -> 28x28\n","        self.pool  = nn.MaxPool2d(2)                  # 28x28 -> 14x14\n","        self.drop1 = nn.Dropout(0.25)\n","        self.fc1   = nn.Linear(64 * 14 * 14, 128)\n","        self.drop2 = nn.Dropout(0.5)\n","        self.fc2   = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.drop1(x)\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = self.drop2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","model = SimpleCNN().to(device=device, dtype=dtype)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n","\n","# --- Training ---\n","def train_one_epoch(loader):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    start = time.perf_counter()\n","    for xb, yb in loader:\n","        xb = xb.to(device=device, dtype=dtype, non_blocking=True)\n","        yb = yb.to(device=device, non_blocking=True)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        logits = model(xb)\n","        loss = criterion(logits, yb)\n","        loss.backward()\n","\n","        if device_choice == \"tpu\":\n","            # XLA-aware optimizer step\n","            xm.optimizer_step(optimizer, barrier=True)\n","            xm.mark_step()\n","        else:\n","            optimizer.step()\n","\n","        running_loss += loss.item() * xb.size(0)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == yb).sum().item()\n","        total += yb.size(0)\n","\n","    end = time.perf_counter()\n","    avg_loss = running_loss / max(total, 1)\n","    acc = correct / max(total, 1)\n","    return avg_loss, acc, end - start\n","\n","# --- Evaluation (Inference on test set) ---\n","@torch.no_grad()\n","def evaluate(loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    start = time.perf_counter()\n","    for xb, yb in loader:\n","        xb = xb.to(device=device, dtype=dtype, non_blocking=True)\n","        yb = yb.to(device=device, non_blocking=True)\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == yb).sum().item()\n","        total += yb.size(0)\n","\n","        if device_choice == \"tpu\":\n","            # ensure execution advances on TPU\n","            xm.mark_step()\n","    end = time.perf_counter()\n","\n","    acc = correct / max(total, 1)\n","    elapsed = end - start\n","    tok_per_s = total / elapsed if elapsed > 0 else float(\"nan\")  # samples/sec\n","    return acc, elapsed, tok_per_s\n","\n","print(f\"Device: {device_choice.upper()}  |  dtype: {dtype}  |  epochs: {epochs}\")\n","for ep in range(1, epochs + 1):\n","    tr_loss, tr_acc, tr_time = train_one_epoch(train_loader)\n","    print(f\"[Epoch {ep}] Train loss: {tr_loss:.4f}  |  Train acc: {tr_acc*100:.2f}%  |  Time: {tr_time:.2f}s\")\n","\n","# Test-time inference timing\n","test_acc, inf_time, samples_per_s = evaluate(test_loader)\n","print(f\"\\nTest accuracy: {test_acc*100:.2f}%\")\n","print(f\"Inference time (test set): {inf_time:.3f} s for {len(test_ds)} samples\")\n","print(f\"Throughput: {samples_per_s:.2f} samples/s on {device_choice.upper()}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Gradient Boosting"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:03:47.335199Z","iopub.status.busy":"2025-10-30T19:03:47.334716Z","iopub.status.idle":"2025-10-30T19:04:14.789231Z","shell.execute_reply":"2025-10-30T19:04:14.788216Z","shell.execute_reply.started":"2025-10-30T19:03:47.335172Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset: wine  |  Train: (142, 13)  |  Test: (36, 13)\n","Implementation: CatBoost (CPU)\n","\n","Device choice: CPU\n","Train time: 25.541 s on CatBoost (CPU)\n","Inference time: 0.003 s for 36 samples\n","Throughput: 13076.07 samples/s  |  0.076 ms/sample\n","Test accuracy: 97.22%\n"]}],"source":["# pip install -q scikit-learn catboost\n","\n","import time\n","import numpy as np\n","from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from catboost import CatBoostClassifier, Pool\n","\n","# --- CONFIG ---\n","device_choice = \"cpu\"   # \"cpu\" | \"cuda\" | \"tpu\"\n","iterations = 1000\n","depth = 10\n","learning_rate = 0.1\n","random_state = 42\n","test_size = 0.2\n","\n","# --- Load dataset ---\n","wine = load_wine()\n","X = wine.data\n","y = wine.target\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=test_size, random_state=random_state, stratify=y\n",")\n","print(f\"Dataset: wine  |  Train: {X_train.shape}  |  Test: {X_test.shape}\")\n","\n","# --- Select device ---\n","if device_choice == \"tpu\":\n","    print(\"TPU not supported for gradient boosting. Running on CPU.\")\n","    device_choice = \"cpu\"\n","\n","# --- Initialize CatBoost ---\n","# CatBoost uses 'task_type' to select between CPU and GPU automatically\n","clf = CatBoostClassifier(\n","    iterations=iterations,\n","    depth=depth,\n","    learning_rate=learning_rate,\n","    loss_function=\"MultiClass\",\n","    task_type=\"GPU\" if device_choice == \"cuda\" else \"CPU\",\n","    random_seed=random_state,\n","    verbose=False,\n",")\n","\n","print(f\"Implementation: CatBoost ({'GPU' if device_choice == 'cuda' else 'CPU'})\")\n","\n","# --- Train ---\n","train_pool = Pool(X_train, y_train)\n","start_train = time.perf_counter()\n","clf.fit(train_pool)\n","end_train = time.perf_counter()\n","train_time = end_train - start_train\n","\n","# --- Inference ---\n","test_pool = Pool(X_test, y_test)\n","start_inf = time.perf_counter()\n","y_pred = clf.predict(test_pool)\n","end_inf = time.perf_counter()\n","inf_time = end_inf - start_inf\n","\n","# --- Metrics ---\n","acc = accuracy_score(y_test, y_pred)\n","samples_per_s = len(y_test) / inf_time if inf_time > 0 else float(\"nan\")\n","ms_per_sample = inf_time / len(y_test) * 1000.0\n","\n","print(f\"\\nDevice choice: {device_choice.upper()}\")\n","print(f\"Train time: {train_time:.3f} s on CatBoost ({device_choice.upper()})\")\n","print(f\"Inference time: {inf_time:.3f} s for {len(y_test)} samples\")\n","print(f\"Throughput: {samples_per_s:.2f} samples/s  |  {ms_per_sample:.3f} ms/sample\")\n","print(f\"Test accuracy: {acc*100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### LLM Inference"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:19:11.090515Z","iopub.status.busy":"2025-10-30T19:19:11.090284Z","iopub.status.idle":"2025-10-30T19:20:55.162447Z","shell.execute_reply":"2025-10-30T19:20:55.160408Z","shell.execute_reply.started":"2025-10-30T19:19:11.090494Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==4.41.2\n","  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.19.1)\n","Collecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.2)\n","  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2025.9.18)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.5)\n","Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n","  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2025.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.10)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (2025.2.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (2022.2.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (2.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.8.3)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.2) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.2) (2022.2.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.41.2) (1.4.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.41.2) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.41.2) (2024.2.0)\n","Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 1.0.0rc2\n","    Uninstalling huggingface-hub-1.0.0rc2:\n","      Successfully uninstalled huggingface-hub-1.0.0rc2\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.2\n","    Uninstalling tokenizers-0.21.2:\n","      Successfully uninstalled tokenizers-0.21.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.53.3\n","    Uninstalling transformers-4.53.3:\n","      Successfully uninstalled transformers-4.53.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n","gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.19.1 transformers-4.41.2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c9192ff729247678ea75e0812d7548a","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff8ed637f0a64645900300a4b55b2e6a","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1cd138d21c5e436dbd422a7598960663","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7432fef6d2bc4343a686c00d4c4afbaf","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loading model on CPU...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7baaa51a126476697b95c44e1b21b0d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcf4f15cec9d44bab086f6dbad72692c","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5aab1920b90b4802a11cd6fedc2d1219","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","--- Generating response ---\n","\n","--- Model Output ---\n","\n","MLOps is a process that involves automating the deployment, monitoring, and scaling of machine learning (ML) models. It is a critical component of modern data science and machine learning workflows. MLOps is a holistic approach that involves the following steps:\n","\n","1. Model Development: The first step in MLOps is model development. This involves creating a model that can be trained on the data and deployed to the production environment.\n","\n","2. Model Deployment: Once the model is developed, it needs to be deployed to the production environment. This involves configuring the infrastructure, setting up the environment, and deploying the model.\n","\n","3. Monitoring and Scaling: MLOps involves monitoring and scaling the model to ensure that it is performing optimally. This involves monitoring the model's performance, identifying issues, and scaling the model to meet the demand.\n","\n","3 Real-World Examples:\n","\n","1. Net\n","\n","--- Stats ---\n","Prompt tokens: 55\n","Generated tokens: 200\n","Total time: 60.423 s\n","Throughput: 3.31 tokens/s on CPU\n"]}],"source":["# pip install -q \"transformers>=4.41\" \"torch>=2.1\" sentencepiece\n","# Optional: pip install -q bitsandbytes accelerate  # for 4-bit GPU quantization\n","# For TPU: pip install -q torch_xla[tpu]~=2.1\n","!pip install \"transformers==4.41.2\"\n","\n","import time\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# --- CONFIG ---\n","MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","QUERY = \"Explain MLOps briefly and give 3 real-world examples.\"\n","device_choice = \"cpu\"   # \"cpu\" | \"cuda\" | \"tpu\"\n","\n","# --- DEVICE SETUP ---\n","if device_choice not in [\"cpu\", \"cuda\", \"tpu\"]:\n","    raise ValueError(\"device_choice must be one of 'cpu', 'cuda', or 'tpu'.\")\n","\n","if device_choice == \"cuda\":\n","    if not torch.cuda.is_available():\n","        raise EnvironmentError(\"CUDA not available — please switch to CPU or TPU.\")\n","    device = torch.device(\"cuda\")\n","elif device_choice == \"tpu\":\n","    try:\n","        import torch_xla.core.xla_model as xm\n","        device = xm.xla_device()\n","        print(\"Using TPU device.\")\n","    except Exception as e:\n","        raise EnvironmentError(\"TPU runtime not found. Did you enable TPU in Colab?\") from e\n","else:\n","    device = torch.device(\"cpu\")\n","\n","# --- LOAD TOKENIZER SAFELY (no chat-template fetch) ---\n","tokenizer = AutoTokenizer.from_pretrained(\n","    MODEL_ID,\n","    use_fast=True,\n","    trust_remote_code=True,\n","    local_files_only=False,\n",")\n","# Prevent future chat-template lookup attempts\n","if hasattr(tokenizer, \"_set_chat_template\"):\n","    tokenizer._set_chat_template(None, \"remove\")\n","\n","# --- LOAD MODEL ON SELECTED DEVICE ---\n","print(f\"Loading model on {device_choice.upper()}...\")\n","\n","dtype = torch.float16 if device_choice == \"cuda\" else torch.float32\n","\n","if device_choice == \"cuda\":\n","    try:\n","        from transformers import BitsAndBytesConfig\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            quantization_config=bnb_config,\n","            device_map={\"\": \"cuda\"},\n","            torch_dtype=torch.float16,\n","            trust_remote_code=True,\n","        )\n","        print(\"Model loaded in 4-bit quantized GPU mode.\")\n","    except Exception:\n","        print(\"bitsandbytes not installed — loading model in full precision.\")\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            torch_dtype=torch.float16,\n","            trust_remote_code=True,\n","        ).to(device)\n","elif device_choice == \"tpu\":\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=torch.float32,\n","        trust_remote_code=True,\n","    ).to(device)\n","else:\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=torch.float32,\n","        trust_remote_code=True,\n","    ).to(device)\n","\n","model.eval()\n","\n","# --- PROMPT PREPARATION ---\n","system_prompt = \"You are a concise and knowledgeable AI assistant.\"\n","prompt = (\n","    f\"<|system|>\\n{system_prompt}\\n</s>\\n\"\n","    f\"<|user|>\\n{QUERY}\\n</s>\\n\"\n","    f\"<|assistant|>\\n\"\n",")\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","# --- GENERATION ---\n","gen_kwargs = dict(max_new_tokens=200, temperature=0.2, top_p=0.95, do_sample=True)\n","\n","print(\"\\n--- Generating response ---\")\n","start = time.perf_counter()\n","with torch.no_grad():\n","    if device_choice == \"tpu\":\n","        output_ids = model.generate(**inputs, **gen_kwargs)\n","        import torch_xla.core.xla_model as xm\n","        xm.mark_step()\n","    else:\n","        output_ids = model.generate(**inputs, **gen_kwargs)\n","end = time.perf_counter()\n","\n","# --- DECODE ---\n","generated_text = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n","print(\"\\n--- Model Output ---\\n\")\n","print(generated_text.strip())\n","\n","# --- TIMING ---\n","elapsed_s = end - start\n","generated_tokens = output_ids.shape[1] - inputs[\"input_ids\"].shape[1]\n","tps = generated_tokens / elapsed_s if elapsed_s > 0 else float(\"nan\")\n","\n","print(\"\\n--- Stats ---\")\n","print(f\"Prompt tokens: {inputs['input_ids'].shape[1]}\")\n","print(f\"Generated tokens: {generated_tokens}\")\n","print(f\"Total time: {elapsed_s:.3f} s\")\n","print(f\"Throughput: {tps:.2f} tokens/s on {device_choice.upper()}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## GPU Experiments"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Networks"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:40:58.139655Z","iopub.status.busy":"2025-10-30T19:40:58.139436Z","iopub.status.idle":"2025-10-30T19:41:26.392899Z","shell.execute_reply":"2025-10-30T19:41:26.391892Z","shell.execute_reply.started":"2025-10-30T19:40:58.139629Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 9.91M/9.91M [00:01<00:00, 5.85MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 154kB/s]\n","100%|██████████| 1.65M/1.65M [00:01<00:00, 1.46MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 7.68MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Device: CUDA  |  dtype: torch.float32  |  epochs: 1\n","[Epoch 1] Train loss: 0.2370  |  Train acc: 92.73%  |  Time: 9.00s\n","\n","Test accuracy: 98.35%\n","Inference time (test set): 1.315 s for 10000 samples\n","Throughput: 7605.74 samples/s on CUDA\n"]}],"source":["# pip install -q \"torch>=2.1\" \"torchvision>=0.16\" \"transformers>=4.41\"  # transformers not required; kept for parity\n","# For TPU: pip install -q torch_xla[tpu]~=2.1\n","\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","# --- CONFIG ---\n","device_choice = \"cuda\"   # \"cpu\" | \"cuda\" | \"tpu\"\n","batch_size = 128\n","epochs = 1              # keep small for quick sanity runs; increase as needed\n","lr = 1e-3\n","seed = 42\n","\n","torch.manual_seed(seed)\n","\n","# --- Device selection & checks ---\n","xm = None\n","if device_choice not in [\"cpu\", \"cuda\", \"tpu\"]:\n","    raise ValueError(\"device_choice must be 'cpu', 'cuda', or 'tpu'.\")\n","\n","if device_choice == \"cuda\":\n","    if not torch.cuda.is_available():\n","        raise EnvironmentError(\"CUDA not available but 'cuda' was requested.\")\n","    device = torch.device(\"cuda\")\n","    torch.backends.cudnn.benchmark = True\n","elif device_choice == \"tpu\":\n","    try:\n","        import torch_xla.core.xla_model as xm\n","        device = xm.xla_device()\n","    except Exception as e:\n","        raise EnvironmentError(\n","            \"TPU/XLA not available. Ensure TPU runtime and torch_xla are installed.\"\n","        ) from e\n","else:\n","    device = torch.device(\"cpu\")\n","\n","dtype = torch.float32  # use fp32 for reliability across backends\n","\n","# --- Data ---\n","tfm = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,)),  # standard MNIST normalization\n","])\n","\n","train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n","test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=tfm)\n","\n","# sensible loader defaults across devices\n","num_workers = 2 if device_choice in [\"cpu\", \"cuda\"] else 0\n","pin_memory = True if device_choice == \"cuda\" else False\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n","                          num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n","test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n","                          num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n","\n","# --- Model ---\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)   # 28x28 -> 28x28\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 28x28 -> 28x28\n","        self.pool  = nn.MaxPool2d(2)                  # 28x28 -> 14x14\n","        self.drop1 = nn.Dropout(0.25)\n","        self.fc1   = nn.Linear(64 * 14 * 14, 128)\n","        self.drop2 = nn.Dropout(0.5)\n","        self.fc2   = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.drop1(x)\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = self.drop2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","model = SimpleCNN().to(device=device, dtype=dtype)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n","\n","# --- Training ---\n","def train_one_epoch(loader):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    start = time.perf_counter()\n","    for xb, yb in loader:\n","        xb = xb.to(device=device, dtype=dtype, non_blocking=True)\n","        yb = yb.to(device=device, non_blocking=True)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        logits = model(xb)\n","        loss = criterion(logits, yb)\n","        loss.backward()\n","\n","        if device_choice == \"tpu\":\n","            # XLA-aware optimizer step\n","            xm.optimizer_step(optimizer, barrier=True)\n","            xm.mark_step()\n","        else:\n","            optimizer.step()\n","\n","        running_loss += loss.item() * xb.size(0)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == yb).sum().item()\n","        total += yb.size(0)\n","\n","    end = time.perf_counter()\n","    avg_loss = running_loss / max(total, 1)\n","    acc = correct / max(total, 1)\n","    return avg_loss, acc, end - start\n","\n","# --- Evaluation (Inference on test set) ---\n","@torch.no_grad()\n","def evaluate(loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    start = time.perf_counter()\n","    for xb, yb in loader:\n","        xb = xb.to(device=device, dtype=dtype, non_blocking=True)\n","        yb = yb.to(device=device, non_blocking=True)\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == yb).sum().item()\n","        total += yb.size(0)\n","\n","        if device_choice == \"tpu\":\n","            # ensure execution advances on TPU\n","            xm.mark_step()\n","    end = time.perf_counter()\n","\n","    acc = correct / max(total, 1)\n","    elapsed = end - start\n","    tok_per_s = total / elapsed if elapsed > 0 else float(\"nan\")  # samples/sec\n","    return acc, elapsed, tok_per_s\n","\n","print(f\"Device: {device_choice.upper()}  |  dtype: {dtype}  |  epochs: {epochs}\")\n","for ep in range(1, epochs + 1):\n","    tr_loss, tr_acc, tr_time = train_one_epoch(train_loader)\n","    print(f\"[Epoch {ep}] Train loss: {tr_loss:.4f}  |  Train acc: {tr_acc*100:.2f}%  |  Time: {tr_time:.2f}s\")\n","\n","# Test-time inference timing\n","test_acc, inf_time, samples_per_s = evaluate(test_loader)\n","print(f\"\\nTest accuracy: {test_acc*100:.2f}%\")\n","print(f\"Inference time (test set): {inf_time:.3f} s for {len(test_ds)} samples\")\n","print(f\"Throughput: {samples_per_s:.2f} samples/s on {device_choice.upper()}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Gradient Boosting"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:41:34.683564Z","iopub.status.busy":"2025-10-30T19:41:34.682879Z","iopub.status.idle":"2025-10-30T19:42:10.197533Z","shell.execute_reply":"2025-10-30T19:42:10.196718Z","shell.execute_reply.started":"2025-10-30T19:41:34.683536Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset: wine  |  Train: (142, 13)  |  Test: (36, 13)\n","Implementation: CatBoost (GPU)\n","\n","Device choice: CUDA\n","Train time: 33.910 s on CatBoost (CUDA)\n","Inference time: 0.003 s for 36 samples\n","Throughput: 13490.20 samples/s  |  0.074 ms/sample\n","Test accuracy: 100.00%\n"]}],"source":["# pip install -q scikit-learn catboost\n","\n","import time\n","import numpy as np\n","from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from catboost import CatBoostClassifier, Pool\n","\n","# --- CONFIG ---\n","device_choice = \"cuda\"   # \"cpu\" | \"cuda\" | \"tpu\"\n","iterations = 1000\n","depth = 10\n","learning_rate = 0.1\n","random_state = 42\n","test_size = 0.2\n","\n","# --- Load dataset ---\n","wine = load_wine()\n","X = wine.data\n","y = wine.target\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=test_size, random_state=random_state, stratify=y\n",")\n","print(f\"Dataset: wine  |  Train: {X_train.shape}  |  Test: {X_test.shape}\")\n","\n","# --- Select device ---\n","if device_choice == \"tpu\":\n","    print(\"TPU not supported for gradient boosting. Running on CPU.\")\n","    device_choice = \"cpu\"\n","\n","# --- Initialize CatBoost ---\n","# CatBoost uses 'task_type' to select between CPU and GPU automatically\n","clf = CatBoostClassifier(\n","    iterations=iterations,\n","    depth=depth,\n","    learning_rate=learning_rate,\n","    loss_function=\"MultiClass\",\n","    task_type=\"GPU\" if device_choice == \"cuda\" else \"CPU\",\n","    random_seed=random_state,\n","    verbose=False,\n",")\n","\n","print(f\"Implementation: CatBoost ({'GPU' if device_choice == 'cuda' else 'CPU'})\")\n","\n","# --- Train ---\n","train_pool = Pool(X_train, y_train)\n","start_train = time.perf_counter()\n","clf.fit(train_pool)\n","end_train = time.perf_counter()\n","train_time = end_train - start_train\n","\n","# --- Inference ---\n","test_pool = Pool(X_test, y_test)\n","start_inf = time.perf_counter()\n","y_pred = clf.predict(test_pool)\n","end_inf = time.perf_counter()\n","inf_time = end_inf - start_inf\n","\n","# --- Metrics ---\n","acc = accuracy_score(y_test, y_pred)\n","samples_per_s = len(y_test) / inf_time if inf_time > 0 else float(\"nan\")\n","ms_per_sample = inf_time / len(y_test) * 1000.0\n","\n","print(f\"\\nDevice choice: {device_choice.upper()}\")\n","print(f\"Train time: {train_time:.3f} s on CatBoost ({device_choice.upper()})\")\n","print(f\"Inference time: {inf_time:.3f} s for {len(y_test)} samples\")\n","print(f\"Throughput: {samples_per_s:.2f} samples/s  |  {ms_per_sample:.3f} ms/sample\")\n","print(f\"Test accuracy: {acc*100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### LLM Inference"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:44:26.373392Z","iopub.status.busy":"2025-10-30T19:44:26.372622Z","iopub.status.idle":"2025-10-30T19:45:01.763407Z","shell.execute_reply":"2025-10-30T19:45:01.762633Z","shell.execute_reply.started":"2025-10-30T19:44:26.373362Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==4.41.2\n","  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.19.1)\n","Collecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.2)\n","  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2025.9.18)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.5)\n","Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n","  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2025.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.10)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (2025.2.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (2022.2.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.41.2) (2.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.8.3)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.2) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.41.2) (2022.2.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.41.2) (1.4.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.41.2) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.41.2) (2024.2.0)\n","Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 1.0.0rc2\n","    Uninstalling huggingface-hub-1.0.0rc2:\n","      Successfully uninstalled huggingface-hub-1.0.0rc2\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.2\n","    Uninstalling tokenizers-0.21.2:\n","      Successfully uninstalled tokenizers-0.21.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.53.3\n","    Uninstalling transformers-4.53.3:\n","      Successfully uninstalled transformers-4.53.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n","gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.19.1 transformers-4.41.2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fefe15bc4752470880cb86d8794882ed","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5acdf9460aaa46a8b24504bb4ae60793","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0afea09d2f974d0e92c94637d95d1e4a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a0c0835b4ea498ba1db866ca5096c86","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loading model on CUDA...\n","bitsandbytes not installed — loading model in full precision.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9e11de14b784e82bf87df68cb45a57e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0311bdf43d854d04bd06cf0ea10a0ce9","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee5d6cc69dfb43db81bc448209f5f223","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","--- Generating response ---\n","\n","--- Model Output ---\n","\n","MLOps is a process that automates the deployment, monitoring, and maintenance of machine learning (ML) models. It involves the following steps:\n","\n","1. Model development: The first step in MLOps is to develop the ML model. This involves collecting data, defining the model architecture, selecting the appropriate algorithms, and training the model.\n","\n","2. Deployment: Once the model is developed, it needs to be deployed to production. This involves configuring the infrastructure, setting up the environment, and deploying the model.\n","\n","3. Monitoring: MLOps involves monitoring the model's performance and identifying any issues. This involves collecting metrics, logging, and monitoring the model's behavior.\n","\n","4. Maintenance: MLOps involves maintaining the model over time. This involves updating the model architecture, fixing bugs, and ensuring that the model is performing optimally.\n","\n","Here are three real-world\n","\n","--- Stats ---\n","Prompt tokens: 55\n","Generated tokens: 200\n","Total time: 6.237 s\n","Throughput: 32.07 tokens/s on CUDA\n"]}],"source":["# pip install -q \"transformers>=4.41\" \"torch>=2.1\" sentencepiece\n","# Optional: pip install -q bitsandbytes accelerate  # for 4-bit GPU quantization\n","# For TPU: pip install -q torch_xla[tpu]~=2.1\n","!pip install \"transformers==4.41.2\"\n","\n","import time\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# --- CONFIG ---\n","MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","QUERY = \"Explain MLOps briefly and give 3 real-world examples.\"\n","device_choice = \"cuda\"   # \"cpu\" | \"cuda\" | \"tpu\"\n","\n","# --- DEVICE SETUP ---\n","if device_choice not in [\"cpu\", \"cuda\", \"tpu\"]:\n","    raise ValueError(\"device_choice must be one of 'cpu', 'cuda', or 'tpu'.\")\n","\n","if device_choice == \"cuda\":\n","    if not torch.cuda.is_available():\n","        raise EnvironmentError(\"CUDA not available — please switch to CPU or TPU.\")\n","    device = torch.device(\"cuda\")\n","elif device_choice == \"tpu\":\n","    try:\n","        import torch_xla.core.xla_model as xm\n","        device = xm.xla_device()\n","        print(\"Using TPU device.\")\n","    except Exception as e:\n","        raise EnvironmentError(\"TPU runtime not found. Did you enable TPU in Colab?\") from e\n","else:\n","    device = torch.device(\"cpu\")\n","\n","# --- LOAD TOKENIZER SAFELY (no chat-template fetch) ---\n","tokenizer = AutoTokenizer.from_pretrained(\n","    MODEL_ID,\n","    use_fast=True,\n","    trust_remote_code=True,\n","    local_files_only=False,\n",")\n","# Prevent future chat-template lookup attempts\n","if hasattr(tokenizer, \"_set_chat_template\"):\n","    tokenizer._set_chat_template(None, \"remove\")\n","\n","# --- LOAD MODEL ON SELECTED DEVICE ---\n","print(f\"Loading model on {device_choice.upper()}...\")\n","\n","dtype = torch.float16 if device_choice == \"cuda\" else torch.float32\n","\n","if device_choice == \"cuda\":\n","    try:\n","        from transformers import BitsAndBytesConfig\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            quantization_config=bnb_config,\n","            device_map={\"\": \"cuda\"},\n","            torch_dtype=torch.float16,\n","            trust_remote_code=True,\n","        )\n","        print(\"Model loaded in 4-bit quantized GPU mode.\")\n","    except Exception:\n","        print(\"bitsandbytes not installed — loading model in full precision.\")\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            torch_dtype=torch.float16,\n","            trust_remote_code=True,\n","        ).to(device)\n","elif device_choice == \"tpu\":\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=torch.float32,\n","        trust_remote_code=True,\n","    ).to(device)\n","else:\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        torch_dtype=torch.float32,\n","        trust_remote_code=True,\n","    ).to(device)\n","\n","model.eval()\n","\n","# --- PROMPT PREPARATION ---\n","system_prompt = \"You are a concise and knowledgeable AI assistant.\"\n","prompt = (\n","    f\"<|system|>\\n{system_prompt}\\n</s>\\n\"\n","    f\"<|user|>\\n{QUERY}\\n</s>\\n\"\n","    f\"<|assistant|>\\n\"\n",")\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","# --- GENERATION ---\n","gen_kwargs = dict(max_new_tokens=200, temperature=0.2, top_p=0.95, do_sample=True)\n","\n","print(\"\\n--- Generating response ---\")\n","start = time.perf_counter()\n","with torch.no_grad():\n","    if device_choice == \"tpu\":\n","        output_ids = model.generate(**inputs, **gen_kwargs)\n","        import torch_xla.core.xla_model as xm\n","        xm.mark_step()\n","    else:\n","        output_ids = model.generate(**inputs, **gen_kwargs)\n","end = time.perf_counter()\n","\n","# --- DECODE ---\n","generated_text = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n","print(\"\\n--- Model Output ---\\n\")\n","print(generated_text.strip())\n","\n","# --- TIMING ---\n","elapsed_s = end - start\n","generated_tokens = output_ids.shape[1] - inputs[\"input_ids\"].shape[1]\n","tps = generated_tokens / elapsed_s if elapsed_s > 0 else float(\"nan\")\n","\n","print(\"\\n--- Stats ---\")\n","print(f\"Prompt tokens: {inputs['input_ids'].shape[1]}\")\n","print(f\"Generated tokens: {generated_tokens}\")\n","print(f\"Total time: {elapsed_s:.3f} s\")\n","print(f\"Throughput: {tps:.2f} tokens/s on {device_choice.upper()}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## TPU Experiments"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Networks"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-10-30T19:45:56.207528Z","iopub.status.busy":"2025-10-30T19:45:56.207388Z","iopub.status.idle":"2025-10-30T19:47:25.790919Z","shell.execute_reply":"2025-10-30T19:47:25.789514Z","shell.execute_reply.started":"2025-10-30T19:45:56.207513Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n","  warnings.warn(\n","/tmp/ipykernel_10/1744856404.py:34: DeprecationWarning: Use torch_xla.device instead\n","  device = xm.xla_device()\n","WARNING: Logging before InitGoogle() is written to STDERR\n","E0000 00:00:1761853576.621951      10 common_lib.cc:648] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n","=== Source Location Trace: === \n","learning/45eac/tfrc/runtime/common_lib.cc:238\n","100%|██████████| 9.91M/9.91M [00:00<00:00, 38.7MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 1.02MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 9.49MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 9.05MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Device: TPU  |  dtype: torch.float32  |  epochs: 1\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10/1744856404.py:108: DeprecationWarning: Use torch_xla.sync instead\n","  xm.mark_step()\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 1] Train loss: 0.2437  |  Train acc: 92.51%  |  Time: 48.88s\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10/1744856404.py:140: DeprecationWarning: Use torch_xla.sync instead\n","  xm.mark_step()\n"]},{"name":"stdout","output_type":"stream","text":["\n","Test accuracy: 98.25%\n","Inference time (test set): 9.993 s for 10000 samples\n","Throughput: 1000.66 samples/s on TPU\n"]}],"source":["# pip install -q \"torch>=2.1\" \"torchvision>=0.16\" \"transformers>=4.41\"  # transformers not required; kept for parity\n","# For TPU: pip install -q torch_xla[tpu]~=2.1\n","\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","# --- CONFIG ---\n","device_choice = \"tpu\"   # \"cpu\" | \"cuda\" | \"tpu\"\n","batch_size = 128\n","epochs = 1              # keep small for quick sanity runs; increase as needed\n","lr = 1e-3\n","seed = 42\n","\n","torch.manual_seed(seed)\n","\n","# --- Device selection & checks ---\n","xm = None\n","if device_choice not in [\"cpu\", \"cuda\", \"tpu\"]:\n","    raise ValueError(\"device_choice must be 'cpu', 'cuda', or 'tpu'.\")\n","\n","if device_choice == \"cuda\":\n","    if not torch.cuda.is_available():\n","        raise EnvironmentError(\"CUDA not available but 'cuda' was requested.\")\n","    device = torch.device(\"cuda\")\n","    torch.backends.cudnn.benchmark = True\n","elif device_choice == \"tpu\":\n","    try:\n","        import torch_xla.core.xla_model as xm\n","        device = xm.xla_device()\n","    except Exception as e:\n","        raise EnvironmentError(\n","            \"TPU/XLA not available. Ensure TPU runtime and torch_xla are installed.\"\n","        ) from e\n","else:\n","    device = torch.device(\"cpu\")\n","\n","dtype = torch.float32  # use fp32 for reliability across backends\n","\n","# --- Data ---\n","tfm = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,)),  # standard MNIST normalization\n","])\n","\n","train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n","test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=tfm)\n","\n","# sensible loader defaults across devices\n","num_workers = 2 if device_choice in [\"cpu\", \"cuda\"] else 0\n","pin_memory = True if device_choice == \"cuda\" else False\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n","                          num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n","test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n","                          num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n","\n","# --- Model ---\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)   # 28x28 -> 28x28\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 28x28 -> 28x28\n","        self.pool  = nn.MaxPool2d(2)                  # 28x28 -> 14x14\n","        self.drop1 = nn.Dropout(0.25)\n","        self.fc1   = nn.Linear(64 * 14 * 14, 128)\n","        self.drop2 = nn.Dropout(0.5)\n","        self.fc2   = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.drop1(x)\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = self.drop2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","model = SimpleCNN().to(device=device, dtype=dtype)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n","\n","# --- Training ---\n","def train_one_epoch(loader):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    start = time.perf_counter()\n","    for xb, yb in loader:\n","        xb = xb.to(device=device, dtype=dtype, non_blocking=True)\n","        yb = yb.to(device=device, non_blocking=True)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        logits = model(xb)\n","        loss = criterion(logits, yb)\n","        loss.backward()\n","\n","        if device_choice == \"tpu\":\n","            # XLA-aware optimizer step\n","            xm.optimizer_step(optimizer, barrier=True)\n","            xm.mark_step()\n","        else:\n","            optimizer.step()\n","\n","        running_loss += loss.item() * xb.size(0)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == yb).sum().item()\n","        total += yb.size(0)\n","\n","    end = time.perf_counter()\n","    avg_loss = running_loss / max(total, 1)\n","    acc = correct / max(total, 1)\n","    return avg_loss, acc, end - start\n","\n","# --- Evaluation (Inference on test set) ---\n","@torch.no_grad()\n","def evaluate(loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    start = time.perf_counter()\n","    for xb, yb in loader:\n","        xb = xb.to(device=device, dtype=dtype, non_blocking=True)\n","        yb = yb.to(device=device, non_blocking=True)\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == yb).sum().item()\n","        total += yb.size(0)\n","\n","        if device_choice == \"tpu\":\n","            # ensure execution advances on TPU\n","            xm.mark_step()\n","    end = time.perf_counter()\n","\n","    acc = correct / max(total, 1)\n","    elapsed = end - start\n","    tok_per_s = total / elapsed if elapsed > 0 else float(\"nan\")  # samples/sec\n","    return acc, elapsed, tok_per_s\n","\n","print(f\"Device: {device_choice.upper()}  |  dtype: {dtype}  |  epochs: {epochs}\")\n","for ep in range(1, epochs + 1):\n","    tr_loss, tr_acc, tr_time = train_one_epoch(train_loader)\n","    print(f\"[Epoch {ep}] Train loss: {tr_loss:.4f}  |  Train acc: {tr_acc*100:.2f}%  |  Time: {tr_time:.2f}s\")\n","\n","# Test-time inference timing\n","test_acc, inf_time, samples_per_s = evaluate(test_loader)\n","print(f\"\\nTest accuracy: {test_acc*100:.2f}%\")\n","print(f\"Inference time (test set): {inf_time:.3f} s for {len(test_ds)} samples\")\n","print(f\"Throughput: {samples_per_s:.2f} samples/s on {device_choice.upper()}\")\n"]}],"metadata":{"kaggle":{"accelerator":"tpuV5e8","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"}},"nbformat":4,"nbformat_minor":4}
